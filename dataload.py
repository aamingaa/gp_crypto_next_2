'''
æ•°æ®è¯»å–ã€é™é¢‘å¤„ç†å’Œè®¡ç®—æ”¶ç›Šç‡æ¨¡å—
'''

import pandas as pd
import numpy as np
from pathlib import Path
import originalFeature
from scipy.optimize import minimize
import time
import talib as ta
from enum import Enum
import re

import sys
import matplotlib.pyplot as plt
from scipy.stats import zscore, kurtosis, skew, yeojohnson, boxcox
from scipy.stats import tukeylambda, mstats
from sklearn.preprocessing import RobustScaler
from typing import Dict, List, Optional, Tuple, Any
import time
import talib as ta
from enum import Enum
import re
import os
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta

import normDataCheck

import sys
import matplotlib.pyplot as plt
from scipy.stats import zscore, kurtosis, skew, yeojohnson, boxcox
from scipy.stats import tukeylambda, mstats
import zipfile


def data_load(sym: str) -> pd.DataFrame:
    '''æ•°æ®è¯»å–æ¨¡å—'''
    file_name = '/home/etern/crypto/data/merged/merged/' + sym + '-merged-without-rfr-1m.csv'  
    z = pd.read_csv(file_name, index_col=1)[
        ['o', 'h', 'l', 'c', 'vol', 'vol_ccy', 'trades',
               'oi', 'oi_ccy', 'toptrader_count_lsr', 'toptrader_oi_lsr', 'count_lsr',
               'taker_vol_lsr']]
    return z



def removed_zero_vol_dataframe(df):
    """
    æ‰“å°å¹¶ä¸”è¿”å›-
    1. volumeè¿™ä¸€åˆ—ä¸º0çš„è¡Œç»„æˆçš„df
    2. lowè¿™ä¸€åˆ—çš„æœ€å°å€¼
    3. volumeè¿™ä¸€åˆ—çš„æœ€å°å€¼
    5. å»é™¤æ‰volume=0çš„è¡Œçš„dataframe
    -------

    """
    # å°†DataFrameçš„ç´¢å¼•åˆ—è®¾ç½®ä¸º'datetime'
    df.index = pd.to_datetime(df.index)

    # 1. volumeè¿™ä¸€åˆ—ä¸º0çš„è¡Œç»„æˆçš„df
    volume_zero_df = df[df['vol'] == 0]
    print(f"Volumeä¸º0çš„è¡Œç»„æˆçš„DataFrame: {len(volume_zero_df)}")

    # 2. lowè¿™ä¸€åˆ—çš„æœ€å°å€¼
    min_low = df['l'].min()
    print(f"Lowè¿™ä¸€åˆ—çš„æœ€å°å€¼: {min_low}")

    # 3. volumeè¿™ä¸€åˆ—çš„æœ€å°å€¼
    min_volume = df['vol'].min()
    print(f"Volumeè¿™ä¸€åˆ—çš„æœ€å°å€¼: {min_volume}")

    # 5. å»é™¤æ‰volume=0çš„è¡Œçš„dataframe
    removed_zero_vol_df = df[df['vol'] != 0]
    print(f"å»é™¤æ‰Volumeä¸º0çš„è¡Œä¹‹å‰çš„DataFrame length: {len(df)}")
    print(f"å»é™¤æ‰Volumeä¸º0çš„è¡Œä¹‹åçš„DataFrame length: {len(removed_zero_vol_df)}")

    return removed_zero_vol_df


def resample(z: pd.DataFrame, freq: str) -> pd.DataFrame:
    '''
    è¿™æ˜¯ä¸æ”¯æŒvwapçš„ï¼Œé»˜è®¤è¯»å…¥çš„æ•°æ®æ˜¯æ²¡æœ‰turnoverä¿¡æ¯ï¼Œè‡ªç„¶ä¹Ÿæ²¡æœ‰vwapçš„ä¿¡æ¯ï¼Œä¸éœ€è¦è·å–symçš„ä¹˜æ•°
    '''
    if freq != '1min' or freq != '1m':
        z.index = pd.to_datetime(z.index)
        # æ³¨æ„closedå’Œlabelå‚æ•°
        z = z.resample(freq, closed='left', label='left').agg({'o': 'first',
                                                               'h': 'max',
                                                               'l': 'min',
                                                               'c': 'last',
                                                               'vol': 'sum',
                                                               'vol_ccy': 'sum',
                                                               'trades': 'sum',
                                                               'oi': 'last', 
                                                               'oi_ccy': 'last', 
                                                               'toptrader_count_lsr':'last', 
                                                               'toptrader_oi_lsr':'last', 
                                                               'count_lsr':'last',
                                                               'taker_vol_lsr':'last'})
        # æ³¨æ„resampleå,æ¯”å¦‚ä»¥10minä¸ºresampleçš„freqï¼Œ9:00çš„æ•°æ®æ˜¯æŒ‡9:00åˆ°9:10çš„æ•°æ®~~
        z = z.fillna(method='ffill')   
        z.columns = ['o', 'h', 'l', 'c', 'vol', 'vol_ccy','trades',
               'oi', 'oi_ccy', 'toptrader_count_lsr', 'toptrader_oi_lsr', 'count_lsr',
               'taker_vol_lsr']

        # é‡è¦ï¼Œè¿™ä¸ªåˆ æ‰0æˆäº¤çš„æ“ä½œï¼Œä¸èƒ½ç»™5åˆ†é’Ÿä»¥å†…çš„freqè¿›è¡Œæ“ä½œï¼Œå› ä¸ºè¿™ç§æƒ…å†µè¿˜æ˜¯æŒºå®¹æ˜“å‡ºç°æ²¡æœ‰æˆäº¤çš„ï¼Œè¿™ä¼šæ”¹å˜æœ¬èº«çš„åˆ†å¸ƒ
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å¼€å¤´çš„æ•°å€¼éƒ¨åˆ†, åˆ¤æ–­freqçš„å‘¨æœŸ
        match = re.match(r"(\d+)", freq)
        if match:
            int_freq = int(match.group(1))
        if int_freq > 5:
            z = removed_zero_vol_dataframe(z)
    return z

def data_prepare_coarse_grain_rolling_offset(
        sym: str, 
        freq: str,  # é¢„æµ‹å‘¨æœŸï¼Œä¾‹å¦‚ '2h' è¡¨ç¤ºé¢„æµ‹æœªæ¥2å°æ—¶æ”¶ç›Š
        start_date_train: str, 
        end_date_train: str,
        start_date_test: str, 
        end_date_test: str,
        coarse_grain_period: str = '2h',  # ç²—ç²’åº¦ç‰¹å¾æ¡¶å‘¨æœŸ
        feature_lookback_bars: int = 8,    # ç‰¹å¾å›æº¯æ¡¶æ•°ï¼ˆ8ä¸ª2h = 16å°æ—¶ï¼‰
        rolling_step: str = '15min',       # æ»šåŠ¨æ­¥é•¿
        y_train_ret_period: int = 8,       # é¢„æµ‹å‘¨æœŸï¼ˆä»¥coarse_grainä¸ºå•ä½ï¼Œ1è¡¨ç¤º1ä¸ª2hï¼‰
        rolling_w: int = 2000,
        output_format: str = 'ndarry',
        data_dir: str = '',
        read_frequency: str = '',
        timeframe: str = '',
        file_path: Optional[str] = None,
        use_parallel: bool = True,  # æ˜¯å¦ä½¿ç”¨å¹¶è¡Œå¤„ç†
        n_jobs: int = -1,  # å¹¶è¡Œè¿›ç¨‹æ•°ï¼Œ-1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        use_fine_grain_precompute: bool = True,  # æ˜¯å¦ä½¿ç”¨ç»†ç²’åº¦é¢„è®¡ç®—ä¼˜åŒ–
        include_categories: List[str] = None,
        remove_warmup_rows: bool = True,  # æ˜¯å¦åˆ é™¤rollingçª—å£æœªæ»¡çš„å‰rolling_w-1è¡Œ
        predict_label: str = 'norm'  # é¢„æµ‹æ ‡ç­¾ç±»å‹
    ):
    """
    ç²—ç²’åº¦ç‰¹å¾ + ç»†ç²’åº¦æ»šåŠ¨çš„æ•°æ®å‡†å¤‡æ–¹æ³•ï¼ˆä½¿ç”¨offsetå‚æ•°ç‰ˆæœ¬ï¼‰
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - ç‰¹å¾ä½¿ç”¨ç²—ç²’åº¦å‘¨æœŸï¼ˆå¦‚2å°æ—¶ï¼‰èšåˆï¼Œå‡å°‘å™ªå£°
    - ç‰¹å¾çª—å£ä½¿ç”¨å›ºå®šæ—¶é—´é•¿åº¦ï¼ˆå¦‚8ä¸ª2å°æ—¶ = 16å°æ—¶ï¼‰ï¼Œé¢„æµ‹èµ·ç‚¹ä»¥ç»†ç²’åº¦æ­¥é•¿æ»šåŠ¨ï¼ˆå¦‚15åˆ†é’Ÿï¼‰ï¼Œäº§ç”Ÿé«˜é¢‘æ ·æœ¬ï¼Œ**å…³é”®æ”¹è¿›**ï¼šæ¯ä¸ªæ»šåŠ¨æ—¶é—´ç‚¹éƒ½ç‹¬ç«‹è®¡ç®—å…¶ä¸“å±çš„æ»‘åŠ¨çª—å£ç‰¹å¾ï¼Œé¿å…å¤šä¸ªæ ·æœ¬é‡å¤ä½¿ç”¨ç›¸åŒçš„ç²—ç²’åº¦æ¡¶
    - é¢„æµ‹ç›®æ ‡æ˜¯æœªæ¥Nä¸ªç²—ç²’åº¦å‘¨æœŸçš„æ”¶ç›Šï¼ˆå¦‚æœªæ¥2å°æ—¶ï¼‰
    
    å‚æ•°è¯´æ˜ï¼š
    - sym: äº¤æ˜“å¯¹ç¬¦å·
    - freq: ç”¨äºå…¼å®¹ï¼Œå®é™…é¢„æµ‹å‘¨æœŸç”± y_train_ret_period * coarse_grain_period å†³å®š
    - coarse_grain_period: ç²—ç²’åº¦ç‰¹å¾æ¡¶å‘¨æœŸï¼Œå¦‚ '2h', '1h', '30min'
    - feature_lookback_bars: ç‰¹å¾å›æº¯çš„ç²—ç²’åº¦æ¡¶æ•°é‡ï¼ˆå¦‚8è¡¨ç¤º8ä¸ª2hæ¡¶ï¼‰
    - rolling_step: æ»šåŠ¨æ­¥é•¿ï¼Œå¦‚ '15min', '10min', '5min'
    - y_train_ret_period: é¢„æµ‹å‘¨æœŸæ•°ï¼ˆä»¥rolling_stepä¸ºå•ä½ï¼‰
    - remove_warmup_rows: æ˜¯å¦åˆ é™¤rollingçª—å£æœªæ»¡çš„å‰rolling_w-1è¡Œï¼ˆé»˜è®¤Falseä¿ç•™æ‰€æœ‰æ•°æ®ï¼‰
    
    ç¤ºä¾‹åœºæ™¯ï¼ˆæ»‘åŠ¨çª—å£ï¼‰ï¼š
    - coarse_grain_period='2h', feature_lookback_bars=8, rolling_step='15min'
    ä¼˜åŠ¿ï¼š
    - æ¯ä¸ªæ—¶é—´ç‚¹çš„ç‰¹å¾çª—å£éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œé¿å…äº†æ•°æ®æ³„éœ²å’Œæ ·æœ¬ç›¸å…³æ€§é—®é¢˜
    - æ»šåŠ¨æ­¥é•¿å¯ä»¥ä»»æ„è®¾ç½®ï¼Œä¸å—ç²—ç²’åº¦å‘¨æœŸé™åˆ¶
    - ç‰¹å¾æ›´åŠ ç²¾ç»†ï¼Œæ›´èƒ½åæ˜ å®æ—¶å¸‚åœºçŠ¶æ€
    - ä½¿ç”¨offsetå‚æ•°ï¼Œé¿å…æ—¶é—´åç§»é—®é¢˜
    
    è¿”å›ä¸ data_prepare ç›¸åŒçš„æ¥å£
    """
    
    print(f"\n{'='*60}")
    print(f"ç²—ç²’åº¦ç‰¹å¾ + ç»†ç²’åº¦æ»šåŠ¨æ•°æ®å‡†å¤‡ï¼ˆoffsetå‚æ•°ç‰ˆæœ¬ï¼‰")
    print(f"å“ç§: {sym}")
    print(f"ç²—ç²’åº¦å‘¨æœŸ: {coarse_grain_period}, ç‰¹å¾çª—å£ {feature_lookback_bars} Ã— {coarse_grain_period} = {feature_lookback_bars * pd.Timedelta(coarse_grain_period).total_seconds() / 3600:.1f}å°æ—¶")
    print(f"é¢„æµ‹å‘¨æœŸ: {y_train_ret_period} Ã— {rolling_step} = {y_train_ret_period * pd.Timedelta(rolling_step).total_seconds() / 3600:.1f}å°æ—¶")
    print(f"{'='*60}\n")
    
    # ========== ç¬¬ä¸€æ­¥ï¼šè¯»å–åŸå§‹æ•°æ®ï¼ˆç»†ç²’åº¦ï¼‰ ==========
    z_raw = data_load_v2(sym, data_dir=data_dir, start_date=start_date_train, end_date=end_date_test,
                         timeframe=timeframe, read_frequency=read_frequency, file_path=file_path)
    z_raw.index = pd.to_datetime(z_raw.index)


    z_raw = z_raw[(z_raw.index >= pd.to_datetime(start_date_train)) 
                  & (z_raw.index <= pd.to_datetime(end_date_test))]
    
    print(f"è¯»å–åŸå§‹æ•°æ®: {len(z_raw)} è¡Œï¼Œæ—¶é—´èŒƒå›´ {z_raw.index.min()} è‡³ {z_raw.index.max()}")
    
    # ========== ç¬¬äºŒæ­¥ï¼šä½¿ç”¨offsetå‚æ•°é¢„è®¡ç®—ç²—ç²’åº¦æ¡¶ç‰¹å¾ ==========
    coarse_features_dict = {}
    
    # è®¡ç®—éœ€è¦å¤šå°‘ç»„ä¸åŒåç§»çš„resample
    coarse_period_minutes = pd.Timedelta(coarse_grain_period).total_seconds() / 60
    rolling_step_minutes = pd.Timedelta(rolling_step).total_seconds() / 60
    num_offsets = int(coarse_period_minutes / rolling_step_minutes)


    if use_fine_grain_precompute:
        print(f"æ»šåŠ¨æ­¥é•¿: {rolling_step} ({rolling_step_minutes}åˆ†é’Ÿ)")
        print(f"éœ€è¦é¢„è®¡ç®— {num_offsets} ç»„ä¸åŒåç§»çš„ç²—ç²’åº¦æ¡¶")
        
        samples = []
        prediction_horizon_td = pd.Timedelta(rolling_step) * y_train_ret_period
        
        for i in range(num_offsets):
            offset = pd.Timedelta(minutes=i * rolling_step_minutes)
            print(f"\nç»„{i}: åç§» {offset} ...")
            
            # ğŸ”‘ å…³é”®æ”¹è¿›ï¼šä½¿ç”¨offsetå‚æ•°æ›¿ä»£æ—¶é—´ç´¢å¼•åç§»
            z_raw_copy = z_raw.copy()
            original_start = z_raw_copy.index.min()
            original_end = z_raw_copy.index.max()

            coarse_bars = resample_with_offset(
                z_raw_copy, 
                coarse_grain_period, 
                offset=offset,  # ç›´æ¥ä½¿ç”¨offsetå‚æ•°
                closed='left', 
                label='left'
            )
            
            # ğŸ”§ è¿‡æ»¤æ‰è¶…å‡ºåŸå§‹æ•°æ®èŒƒå›´çš„æ¡¶
            coarse_bars = coarse_bars [
                (coarse_bars.index >= original_start) & 
                (coarse_bars.index <= original_end)
            ]

            # è®¡ç®—ç‰¹å¾
            base_feature = originalFeature.BaseFeature(
                coarse_bars.copy(), 
                include_categories=include_categories, 
                rolling_zscore_window=int(rolling_w / num_offsets)
            )

            features_df = base_feature.init_feature_df
            
            # è¯´æ˜ï¼š
            # - z_raw æ˜¯ 15m K çº¿ï¼ˆindex ä¸º open_timeï¼‰ã€‚
            # - coarse_bars æ˜¯å¯¹ z_raw æŒ‰ coarse_grain_periodï¼ˆå¦‚ 2hï¼‰resample(closed='left', label='left', offset=...) å¾—åˆ°çš„ç²—ç²’åº¦æ¡¶ï¼›
            #   å…¶ index åŒæ ·æ˜¯æ¡¶çš„ left labelï¼ˆä¹Ÿå°±æ˜¯æ¡¶çš„ open_time / èµ·å§‹æ—¶åˆ» t0ï¼‰ã€‚
            # - features_df.index (= row_timestamps) è¡¨ç¤ºç²—æ¡¶èµ·å§‹æ—¶åˆ» t0ï¼ˆä¾‹å¦‚ 10:00ï¼‰ã€‚
            #   è¯¥è¡Œç‰¹å¾ä½¿ç”¨çš„æ˜¯åŒºé—´ [t0, t0 + coarse_grain_period) çš„èšåˆç»“æœï¼›å¹¶ä¸” BaseFeature å†…çš„ rolling_zscore_window æ˜¯â€œç²—æ¡¶è¡Œæ•°â€ï¼Œ
            #   pandas rolling(std) é»˜è®¤åŒ…å«å½“å‰è¡Œï¼ˆçª—å£æ˜¯å³å¯¹é½ã€å«å½“å‰ç‚¹ï¼‰ã€‚
            # - decision_timestamps = t0 + coarse_grain_periodï¼ˆä¾‹å¦‚ 12:00ï¼‰ï¼Œä»£è¡¨æ¡¶ç»“æŸ/å†³ç­–æ—¶åˆ»ã€‚
            # - prediction_timestamps = decision_timestamps + rolling_step * y_train_ret_periodï¼ˆä¾‹å¦‚ rolling_step=15m ä¸” y_train_ret_period=8 æ—¶ä¸º 14:00ï¼‰ã€‚
            # - å½“å‰ä»·(ç”¨äº label åˆ†æ¯)å–å†³ç­–ç‚¹åˆšç»“æŸçš„é‚£æ ¹ 15m K çº¿çš„ closeï¼šcurrent_data_timestamps = decision_timestamps - 15m
            #   ï¼ˆä¾‹å¦‚ 11:45ï¼Œå¯¹åº”åŒºé—´ [11:45, 12:00) çš„ closeï¼‰ã€‚
            row_timestamps = features_df.index
           
            # 1. è®¡ç®—å†³ç­–æ—¶é—´ (ç‰©ç†æ—¶é—´ 12:00)
            decision_timestamps = row_timestamps + pd.to_timedelta(coarse_grain_period)

            # 2. ã€æ ¸å¿ƒä¿®æ­£ã€‘è®¡ç®—åœ¨ z_raw (Open Time Index) ä¸­å¯¹åº”çš„â€œå½“å‰è¡Œâ€
            # å¦‚æœå†³ç­–æ—¶é—´æ˜¯ 12:00ï¼Œæˆ‘ä»¬éœ€è¦å– 11:45 å¼€å§‹çš„é‚£æ ¹ K çº¿ (å› ä¸ºå®ƒåœ¨ 12:00 ç»“æŸ)
            lookup_offset = pd.Timedelta(rolling_step) # ä¾‹å¦‚ 15min
            # current_data_timestamps = decision_timestamps - lookup_offset
            current_data_timestamps = decision_timestamps
            
            # å‘é‡åŒ–è®¡ç®—æœªæ¥æ—¶åˆ»
            prediction_timestamps = current_data_timestamps + prediction_horizon_td

            # ==============================================================================
            # ğŸ†• ç¬¬äºŒæ­¥ï¼šå‘é‡åŒ–è·å–é¢„è®¡ç®—å¥½çš„ "å¹³æ»‘ Label"
            # ==============================================================================
            
            # 1. è·å–å½“å‰æ—¶åˆ»çš„ä»·æ ¼ å’Œ æ³¢åŠ¨ç‡
            t_prices = z_raw['c'].reindex(current_data_timestamps)
            o_prices = z_raw['o'].reindex(current_data_timestamps)
            
            t_future_smooth = None
            scaled_label = None
            return_p = None
            
            t_future_smooth = z_raw['c'].reindex(prediction_timestamps).values
            return_p = t_future_smooth / t_prices.values
            scaled_label = np.log(return_p)
                    
            # å°†æ ‡ç­¾æ·»åŠ åˆ°features_df
            features_df['feature_offset'] = offset.total_seconds() / 60  # è½¬æ¢ä¸ºåˆ†é’Ÿ
            features_df['decision_timestamps'] = decision_timestamps
            features_df['current_data_timestamps'] = current_data_timestamps
            features_df['prediction_timestamps'] = prediction_timestamps
            features_df['t_price'] = t_prices.values
            features_df['t_future_price'] = t_future_smooth
            features_df['o_price'] = o_prices.values
            features_df['return_f'] = scaled_label
            features_df['return_p'] = return_p

            valid_mask = ~np.isnan(features_df['return_f'])
            features_df_mask = features_df[valid_mask]

            coarse_features_dict[offset] = features_df_mask
            samples.append(features_df_mask)

            print(f"  âœ“ ç»„{i}å®Œæˆ: {len(features_df)} ä¸ªæ¡¶, {len(features_df.columns)} ä¸ªç‰¹å¾")
        
        print(f"\nâœ“ é¢„è®¡ç®—å®Œæˆ: {num_offsets} ç»„ç²—ç²’åº¦ç‰¹å¾")
        print(f"ä¼˜åŒ–ç­–ç•¥: æ¯ä¸ªæ—¶é—´ç‚¹æ ¹æ®å…¶offseté€‰æ‹©å¯¹åº”ç»„çš„é¢„è®¡ç®—ç‰¹å¾")
    
    # ========== ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆç»†ç²’åº¦æ»šåŠ¨æ—¶é—´ç½‘æ ¼ ==========    
    if len(samples) > 0 and isinstance(samples[0], pd.DataFrame):
        df_samples = pd.concat(samples, axis=0, ignore_index=False, copy=False)
                
        # æ£€æŸ¥å¹¶å¤„ç†é‡å¤çš„æ—¶é—´æˆ³
        if df_samples.index.duplicated().any():
            num_duplicates = df_samples.index.duplicated().sum()
            df_samples = df_samples[~df_samples.index.duplicated(keep='first')]
            print(f"  å‘ç° {num_duplicates} ä¸ªé‡å¤æ—¶é—´æˆ³ âœ“ å»é‡åä¿ç•™ {len(df_samples)} è¡Œï¼ˆä¿ç•™firstï¼‰")

        df_samples.sort_index(inplace=True)
        df_samples.dropna(inplace=True)
    else:
        print(f"  ä½¿ç”¨pd.DataFrameåˆå¹¶{len(samples)}ä¸ªæ ·æœ¬...")
        df_samples = pd.DataFrame(samples)
        df_samples.set_index('timestamp', inplace=True)
        df_samples.sort_index(inplace=True)
    
    print(f"æ ·æœ¬æ—¶é—´èŒƒå›´: {df_samples.index.min()} è‡³ {df_samples.index.max()}, æ ·æœ¬æ•°é‡: {len(df_samples)}")
    
    df_samples['ret_rolling_zscore'] = normDataCheck.norm(df_samples['return_f'].values, window=rolling_w)
    # df_samples['ret_rolling_zscore'] = norm_ret(df_samples['return_f'].values, window=rolling_w)
    remove_warmup_rows = True
            
    # ========== åˆ é™¤rollingçª—å£æœªæ»¡çš„è¡Œï¼ˆå¯é€‰ï¼‰ ==========
    if remove_warmup_rows and len(df_samples) > rolling_w:
        print(f"\nåˆ é™¤å‰ {rolling_w-1} è¡Œï¼ˆrollingçª—å£é¢„çƒ­æœŸï¼‰")
        original_len = len(df_samples)
        df_samples = df_samples.iloc[rolling_w:]
        print(f"   æ•°æ®è¡Œæ•°: {original_len} â†’ {len(df_samples)}")
        print(f"   æ–°çš„æ—¶é—´èŒƒå›´: {df_samples.index.min()} è‡³ {df_samples.index.max()}")

    print(f"return_f - ååº¦: {df_samples['return_f'].skew():.4f}, å³°åº¦: {df_samples['return_f'].kurtosis():.4f}")
    print(f"ret_rolling_zscore - ååº¦: {df_samples['ret_rolling_zscore'].skew():.4f}, å³°åº¦: {df_samples['ret_rolling_zscore'].kurtosis():.4f}")
    
    # ========== åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›† ==========
    # è¯´æ˜ï¼š
    # - è®­ç»ƒé›†æ ·æœ¬çš„æ ‡ç­¾ä½¿ç”¨ [decision_timestamps, prediction_timestamps] åŒºé—´çš„æœªæ¥æ”¶ç›Š
    # - ä¸ºé¿å…è®­ç»ƒæ ·æœ¬åœ¨è®¡ç®—æ ‡ç­¾æ—¶è·¨è¶Šåˆ°æµ‹è¯•åŒºé—´ï¼Œè¿™é‡Œåœ¨ end_date_train å‰é¢„ç•™ 2 * prediction_horizon_td çš„å®‰å…¨è¾¹ç•Œ
    effective_end_train = pd.to_datetime(end_date_train) - 2 * prediction_horizon_td
    train_mask = (df_samples.index >= pd.to_datetime(start_date_train)) & \
                 (df_samples.index < pd.to_datetime(effective_end_train))
    
    test_mask = (df_samples.index >= pd.to_datetime(start_date_test)) & \
                (df_samples.index <= pd.to_datetime(end_date_test))
    
    # æå–ç‰¹å¾åˆ—
    exclude_cols = ['t_price', 'o_price', 't_future_price', 'return_p', 'return_f', 
                   'prediction_timestamps', 'ret_rolling_zscore', 'feature_offset', 'decision_timestamps', 'current_data_timestamps']
    feature_cols = [col for col in df_samples.columns if col not in exclude_cols]
    
    X_all = df_samples[feature_cols]
    X_train = df_samples.loc[train_mask, feature_cols]
    X_test = df_samples.loc[test_mask, feature_cols]
    
    y_train = df_samples.loc[train_mask, 'ret_rolling_zscore']
    y_test = df_samples.loc[test_mask, 'ret_rolling_zscore']
    
    ret_train = df_samples.loc[train_mask, 'return_f']
    ret_test = df_samples.loc[test_mask, 'return_f']
    
    y_p_train_origin = df_samples.loc[train_mask, 'return_p']
    y_p_test_origin = df_samples.loc[test_mask, 'return_p']
    
    open_train = df_samples.loc[train_mask, 'o_price']
    close_train = df_samples.loc[train_mask, 't_price']
    open_test = df_samples.loc[test_mask, 'o_price']
    close_test = df_samples.loc[test_mask, 't_price']
    
    feature_names = feature_cols
    
    # ä¿å­˜è®­ç»ƒé›† / æµ‹è¯•é›†å¯¹åº”çš„æ—¶é—´ç´¢å¼•ï¼Œä¾›åç»­å› å­æ¨¡å—ç²¾ç¡®å¯¹é½ä½¿ç”¨
    train_index = df_samples.index[train_mask]
    test_index = df_samples.index[test_mask]
    
    # æ ¼å¼è½¬æ¢
    if output_format == 'ndarry':
        X_all = X_all.values
        X_train = X_train.values
        X_test = X_test.values
    else:
        raise ValueError(f"output_format åº”ä¸º 'ndarry' æˆ– 'dataframe'ï¼Œå½“å‰ä¸º {output_format}")
    
    # æ„å»º ohlc DataFrame
    ohlc_aligned = pd.DataFrame({
        'c': df_samples['t_price'],
        'close': df_samples['t_price']
    }, index=df_samples.index)
    
    print('æ£€æŸ¥x allæ˜¯ä¸æ˜¯ç­‰äº x trainå’Œy trainç›¸åŠ ï¼Œå†æ£€æŸ¥trainå’Œtestä»¥åŠcloseå’Œopençš„å½¢çŠ¶æ˜¯å¦ä¸€è‡´')
    print(f'æ£€æŸ¥X_allçš„å½¢çŠ¶ {X_all.shape}')
    print(f'æ£€æŸ¥x dataset trainçš„å½¢çŠ¶ {X_train.shape}')
    print(f'æ£€æŸ¥y dataset trainçš„å½¢çŠ¶ {y_train.shape}')
    print(f'æ£€æŸ¥x allæ˜¯ä¸æ˜¯ç­‰äºtrainå’Œtestç›¸åŠ  {len(X_all)},{len(X_test)+len(X_train)}')
    print(f'æ£€æŸ¥open trainçš„å½¢çŠ¶ {open_train.shape}')
    print(f'æ£€æŸ¥close trainçš„å½¢çŠ¶ {close_train.shape}')
    print(f'æ£€æŸ¥x dataset testçš„å½¢çŠ¶ {X_test.shape}')
    print(f'æ£€æŸ¥y dataset testçš„å½¢çŠ¶ {y_test.shape}')
    print(f'æ£€æŸ¥open testçš„å½¢çŠ¶ {open_test.shape}')
    print(f'æ£€æŸ¥close testçš„å½¢çŠ¶ {close_test.shape}')
    print(f'æ£€æŸ¥ohlc_alignedçš„å½¢çŠ¶ {ohlc_aligned.shape}')
    print(f'æ£€æŸ¥y_p_train_originçš„å½¢çŠ¶ {y_p_train_origin.shape}')
    print(f'æ£€æŸ¥y_p_test_originçš„å½¢çŠ¶ {y_p_test_origin.shape}')

    # è¿”å›æ¥å£ä¸ data_prepare åŸºæœ¬ä¿æŒä¸€è‡´ï¼Œæ–°å¢ train_index / test_index ç”¨äºåç»­å¯¹é½å› å­
    return (X_all, X_train, y_train, ret_train, X_test, y_test, ret_test,
            feature_names, open_train, open_test, close_train, close_test,
            df_samples.index, ohlc_aligned, y_p_train_origin, y_p_test_origin,
            train_index, test_index)

# åº”ç”¨æ»šåŠ¨æ ‡å‡†åŒ–åˆ°æ ‡ç­¾
def norm_ret(x, window=2000):
    x = np.log1p(np.asarray(x))
    factors_data = pd.DataFrame(x, columns=['factor'])
    factors_data = factors_data.replace([np.inf, -np.inf, np.nan], 0.0)
    factors_std = factors_data.rolling(window=window, min_periods=1).std()
    factor_value = factors_data / factors_std
    factor_value = factor_value.replace([np.inf, -np.inf, np.nan], 0.0)
    return np.nan_to_num(factor_value).flatten()
    

def data_prepare(sym, freq, start_date_train, end_date_train, start_date_test, end_date_test, y_train_ret_period=1,
                 rolling_w=2000, output_format='ndarry', _compute_transformed_series=False, _check_zscore_window_series=False):
    '''
    å†…ç½®äº†ä¸€äº›å¯¹äºlabelçš„åˆ†æï¼Œæ¯”è¾ƒå…³é”®, ä½†åªéœ€è¦ç ”ç©¶å’Œå¯¹æ¯”æ—¶æ‰ä¼šå¼€å¯

        # å¯¹äºLabelçš„åˆ†æçš„æŒ‡å¯¼ç›®æ ‡ï¼Œæ˜¯å¸Œæœ›å®ƒèƒ½å¤Ÿæ¥è¿‘æ­£æ€åˆ†å¸ƒï¼Œååº¦ï¼Œå³°åº¦æ¥è¿‘äº0
    # æ–¹æ¡ˆ1ï¼Œå…ˆå¯¹log_returnåšclipï¼Œå®Œå…¨å»é™¤äº†outlierï¼Œå†çœ‹ååº¦å³°åº¦ï¼Œå†³å®šåç»­æ˜¯å¦rolling_zscore.
    # æ–¹æ¡ˆ2ï¼Œå…ˆå¯¹log_returnåšrolling_zscore,
    # (rollingçª—å£å€¼æ˜¯2000ï¼Œæš‚æ—¶å½“åšç»éªŒæ€§çš„å‚æ•°ï¼Œå–å€¼çš„è‡ªç”±åº¦æ¥æºäºè©¹æ£®ä¸ç­‰å¼å’Œå¤§æ•°å®šç†ï¼Œéƒ½æ˜¯ç”¨æ•°æ®ç®—å‡ºæ¥çš„)
        # 1. å‚æ•°å’Œå•ç‚¹å¤æ™®çš„å…³ç³»ä¸æ˜ç¡®ï¼Œä½†æ˜¯å’Œå‡ ä¸‡ä¸ªå› å­çš„å¤æ™®åªå’Œï¼Œä»–ä»¬çš„å…³ç³»åº”è¯¥å­˜åœ¨ä¸€å®šçš„å‡¸æ€§ï¼›
        # 2. å‚æ•°çš„è®¾ç½®åº”è¯¥åœ¨ç»´æŒ1çš„å‰æä¸‹å…¼é¡¾å¤§æ•°å®šç†ï¼›
        # 3. samplesæ‹†åˆ†æˆä¸ºå‡ ä¸ªclassåæ ·æœ¬é‡ä»ç„¶ç¬¦åˆå¤§æ•°å®šç†ï¼›
    # å¦‚ä¸Šä¸¤ç§æ–¹æ¡ˆçš„å¯¹æ¯”ï¼Œå½“å‰è®¤ä¸ºæ˜¯åº”è¯¥ç¬¬äºŒç§æ–¹å¼ï¼Œåº”è¯¥æ˜¯èƒ½å¤Ÿä¿ç•™ä¸€éƒ¨åˆ†outlierçš„ä¿¡æ¯ï¼Œç›¸å¯¹å¹³è¡¡çš„å‡è½»outlierçš„å½±å“

    Note - æœ€ç»ˆè¦æŠŠçª—å£è¿˜æ²¡ç§¯ç´¯å®Œå…¨çš„éƒ¨åˆ†ï¼Œåˆ é™¤æ‰è¿™äº›æ ·æœ¬ï¼Œå¦åˆ™ä¼šå½±å“è®­ç»ƒçš„ç»“æœã€‚å¾€å¾€æ˜¯éƒ½ç”Ÿæˆäº†featureä¹‹åï¼Œæœ€åå¤„ç†å¥½labelï¼Œå†åšåˆ‡å‰²ã€‚
    '''

    # -----------------------------------------
    z = data_load(sym)
    # åˆ‡åˆ†æ•°æ®ï¼Œåªå–éœ€è¦çš„éƒ¨åˆ† - train and test
    z.index = pd.to_datetime(z.index)
    print(f'å¼€å§‹å¤„ç† {sym} çš„å†å²æ•°æ®')   
    print(f'len of z before select = {len(z)}')
    z = z[(z.index >= pd.to_datetime(start_date_train)) & (
        z.index <= pd.to_datetime(end_date_test))]  # åªæˆªå–å‚æ•°æŒ‡å®šéƒ¨åˆ†dataframe
    print(f'len of z after select = {len(z)}')
    ohlcva_df = resample(z, freq)

    print(f'len of resample_z = {len(ohlcva_df)}')
    # --------------------------------------------------------
    if _compute_transformed_series:
        # åˆ†ælabelçš„åˆ†å¸ƒï¼Œç”»å‡ºlabelçš„å„ç±»å¤„ç†åçš„ ä¸‰é˜¶çŸ©ï¼Œå››é˜¶çŸ©
        compute_transformed_series(z.c)
    if _check_zscore_window_series:
        # ç”»å›¾ï¼Œå±•ç°å„ç§çª—å£ä¸‹çš„labelçš„log_return
        check_zscore_window_series(z, 'c')
    # --------------------------------------------------------
    print('å¼€å§‹ç”Ÿæˆåˆå§‹ç‰¹å¾')
    base_feature = originalFeature.BaseFeature(ohlcva_df.copy())
    z = base_feature.init_feature_df

    # -----------------------------------------

    # å…³é”® - ç”Ÿæˆretè¿™ä¸€åˆ—ï¼Œè¿™æ˜¯labelæ•°å€¼ï¼Œæ•´ä¸ªå› å­è¯„ä¼°ä½“ç³»çš„åŸºç¡€ï¼Œè¦æ³¨æ„åˆ†ælabelåˆ†å¸ƒçš„skewness, kurtosisç­‰.
    # note - éœ€è¦æŠŠç©ºå€¼å¤„ç†æ‰ï¼Œå› ä¸ºæµ‹è¯•é›†ä¸­çš„æœ€åçš„å‡ ä¸ªç©ºå€¼å¯èƒ½åˆšå¥½å½±å“æµ‹è¯•çš„æŒä»“æ•ˆæœ.
    # æ³¨æ„ä½¿ç”¨æ»‘åŠ¨çª—å£æ—¶ï¼Œå¯¹äºæ²¡å¡«æ»¡çš„åŒºåŸŸï¼Œå’Œæœ€åç©ºç©ºå€¼åŒºåŸŸï¼Œä¹Ÿè¦æœ‰ç±»ä¼¼çš„è€ƒé‡ï¼Œé˜²æ­¢åˆšå¥½ç¢°åˆ°æå€¼labelå¼•èµ·å¤±çœŸå½±å“ã€‚
    print('å¼€å§‹ç”Ÿæˆret')
    z['return_f'] = np.log(z['c']).diff(
        y_train_ret_period).shift(-y_train_ret_period)
    z['return_f'] = z['return_f'].fillna(0)
    z['r'] = np.log(z['c']).diff()
    z['r'] = z['r'].fillna(0)


    # ---æ–¹æ¡ˆ2ï¼Œ å…ˆå¯¹labelåšrolling_zscore---------------
    def norm_ret(x, window=rolling_w):  # ä¸å†ç”¨L2 normï¼Œæ¢å¤åˆ°ä¹‹å‰çš„zscoreï¼Œç„¶åè¿™é‡Œéœ€è¦åšçš„æ˜¯ç»™ä»–å¢åŠ ä¸€ä¸ªå‘¨æœŸ

        # æ³¨æ„è¿™ä¸ªå‡½æ•°æ˜¯ç”¨åœ¨returnä¸Šé¢çš„ï¼Œlog1pæœ€å°çš„æ•°å€¼æ˜¯-1ï¼Œç”¨äºreturnåˆé€‚
        x = np.log1p(np.asarray(x))
        factors_data = pd.DataFrame(x, columns=['factor'])
        factors_data = factors_data.replace([np.inf, -np.inf, np.nan], 0.0)
        # factors_mean = factors_data.rolling(
        #     window=window, min_periods=1).mean()
        factors_std = factors_data.rolling(window=window, min_periods=1).std()
        factor_value = factors_data / factors_std
        factor_value = factor_value.replace([np.inf, -np.inf, np.nan], 0.0)
        # factor_value = factor_value.clip(-6, 6)
        return np.nan_to_num(factor_value).flatten()


    # Note - å…ˆå¼ºè¡Œä½¿ç”¨norm_retçœ‹æ•ˆæœ
    z['ret_rolling_zscore'] = norm_ret(z['return_f'])

    # æ­¤æ—¶ï¼Œæ‰€æœ‰çš„featureså’Œlabelï¼Œéƒ½ç”¨ç›¸åŒçª—å£åšå®Œäº†rollingå¤„ç†ï¼Œä¸ºäº†è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥å¼€å§‹åˆ é™¤æ‰è¿˜æ²¡æœ‰å­˜æ»¡çª—å£çš„é‚£äº›è¡Œäº†ã€‚å»é™¤å‰windowè¡Œ
    # é‡è¦ï¼ï¼ å¦‚æœæ‰§è¡Œå¦‚ä¸‹è¿™å¥ï¼Œä¼šzä¸ä¸Šé¢çš„ohlcva_dfä¸ä¸€è‡´ï¼Œå¯¼è‡´originalFeature.BaseFeature(ohlcva_df)åˆå§‹åŒ–çš„ohlcva_dfä¸åševalçš„feature_dataä¸ä¸€è‡´
    # z = z.iloc[window-1:]


    kept_index = z.index
    ohlcva_df = ohlcva_df.loc[kept_index]
    open_train = ohlcva_df['o'][(ohlcva_df['o'].index >= pd.to_datetime(start_date_train)) & (
            ohlcva_df['o'].index < pd.to_datetime(end_date_train))]
    open_test = ohlcva_df['o'][(ohlcva_df['o'].index >= pd.to_datetime(start_date_test)) & (
            ohlcva_df['o'].index <= pd.to_datetime(end_date_test))]
    close_train = ohlcva_df['c'][(ohlcva_df['c'].index >= pd.to_datetime(start_date_train)) & (
            ohlcva_df['c'].index < pd.to_datetime(end_date_train))]
    close_test = ohlcva_df['c'][(ohlcva_df['c'].index >= pd.to_datetime(start_date_test)) & (
            ohlcva_df['c'].index <= pd.to_datetime(end_date_test))]

    print('ret_rolling_zscore skew = {}'.format(
        z['ret_rolling_zscore'].skew()))
    print('ret_rolling_zscore kurtosis = {}'.format(
        z['ret_rolling_zscore'].kurtosis()))

    print('return skew = {}'.format(z['return_f'].skew()))
    print('return kurtosis = {}'.format(z['return_f'].kurtosis()))


    # åˆ‡åˆ†ä¸ºtrainå’Œtestä¸¤ä¸ªæ•°æ®é›†ï¼Œä½†æ˜¯æ³¨æ„ï¼Œtestæ•°æ®é›†å…¶å®å¸¦å…¥äº†ä¹‹å‰çš„æ•°æ®çš„çª—å£, æ˜¯è¦ç‰¹æ„è¿™ä¹ˆåšçš„ã€‚
    z_train = z[(z.index >= pd.to_datetime(start_date_train)) & (
        z.index < pd.to_datetime(end_date_train))]  # åªæˆªå–å‚æ•°æŒ‡å®šéƒ¨åˆ†dataframe
    z_test = z[(z.index >= pd.to_datetime(start_date_test)) & (
        z.index <= pd.to_datetime(end_date_test))]
    # ------------<label åˆ†æ>-------------------------

    # å¯¹äºLabelçš„åˆ†æçš„æŒ‡å¯¼ç›®æ ‡ï¼Œæ˜¯å¸Œæœ›å®ƒèƒ½å¤Ÿæ¥è¿‘æ­£æ€åˆ†å¸ƒï¼Œååº¦ï¼Œå³°åº¦æ¥è¿‘äº0
    # æ–¹æ¡ˆ1ï¼Œå…ˆå¯¹log_returnåšclipï¼Œå®Œå…¨å»é™¤äº†outlierï¼Œå†çœ‹ååº¦å³°åº¦ï¼Œå†³å®šåç»­æ˜¯å¦rolling_zscore.
    # æ–¹æ¡ˆ2ï¼Œå…ˆå¯¹log_returnåšrolling_zscore,
    # (rollingçª—å£å€¼æ˜¯2000ï¼Œæš‚æ—¶å½“åšç»éªŒæ€§çš„å‚æ•°ï¼Œå–å€¼çš„è‡ªç”±åº¦æ¥æºäºè©¹æ£®ä¸ç­‰å¼å’Œå¤§æ•°å®šç†ï¼Œéƒ½æ˜¯ç”¨æ•°æ®ç®—å‡ºæ¥çš„)
    # 1. å‚æ•°å’Œå•ç‚¹å¤æ™®çš„å…³ç³»ä¸æ˜ç¡®ï¼Œä½†æ˜¯å’Œå‡ ä¸‡ä¸ªå› å­çš„å¤æ™®åªå’Œï¼Œä»–ä»¬çš„å…³ç³»åº”è¯¥å­˜åœ¨ä¸€å®šçš„å‡¸æ€§ï¼›
    # 2. å‚æ•°çš„è®¾ç½®åº”è¯¥åœ¨ç»´æŒ1çš„å‰æä¸‹å…¼é¡¾å¤§æ•°å®šç†ï¼›
    # 3. samplesæ‹†åˆ†æˆä¸ºå‡ ä¸ªclassåæ ·æœ¬é‡ä»ç„¶ç¬¦åˆå¤§æ•°å®šç†ï¼›
    # å¦‚ä¸Šä¸¤ç§æ–¹æ¡ˆçš„å¯¹æ¯”ï¼Œå½“å‰è®¤ä¸ºæ˜¯åº”è¯¥ç¬¬äºŒç§æ–¹å¼ï¼Œåº”è¯¥æ˜¯èƒ½å¤Ÿä¿ç•™ä¸€éƒ¨åˆ†outlierçš„ä¿¡æ¯ï¼Œç›¸å¯¹å¹³è¡¡çš„å‡è½»outlierçš„å½±å“



    if output_format == 'ndarry':
        y_dataset_train = z_train['ret_rolling_zscore'].values
        y_dataset_test = z_test['ret_rolling_zscore'].values
        ret_dataset_train = z_train['return_f'].values
        ret_dataset_test = z_test['return_f'].values
        # é‡è¦ï¼è¦åˆ é™¤æ‰åŒ…å«æœªæ¥ä¿¡æ¯çš„å­—æ®µï¼Œretï¼Œret_rolling_zscore
        z_train.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)
        z_test.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)
        z.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)

        X_all = np.where(np.isnan(z), 0, z)
        X_dataset_train = np.where(np.isnan(z_train), 0, z_train)
        X_dataset_test = np.where(np.isnan(z_test), 0, z_test)

    elif output_format == 'dataframe':
        y_dataset_train = z_train['ret_rolling_zscore']
        y_dataset_test = z_test['ret_rolling_zscore']
        ret_dataset_train = z_train['return_f']
        ret_dataset_test = z_test['return_f']
        # é‡è¦ï¼è¦åˆ é™¤æ‰åŒ…å«æœªæ¥ä¿¡æ¯çš„å­—æ®µï¼Œretï¼Œret_rolling_zscore
        z_train.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)
        z_test.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)
        z.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)

        X_all = z.fillna(0)
        X_dataset_train = z_train.fillna(0)
        X_dataset_test = z_test.fillna(0)

    else:
        print(
            'output_format of data_prepare should be "ndarry" or "dataframe", pls check it ')
        exit(1)

    feature_names = z_train.columns

    print('æ£€æŸ¥x allæ˜¯ä¸æ˜¯ç­‰äº x trainå’Œy trianç›¸åŠ ï¼Œå†æ£€æŸ¥trianå’Œtestä»¥åŠcloseå’Œopençš„å½¢çŠ¶æ˜¯å¦ä¸€è‡´')
    # X_all ä¸“é—¨æ˜¯ä¸ºåšbatch predictionçš„æ—¶å€™ï¼Œè¦ç”¨X_allç”Ÿæˆtesté›†è¦ç”¨åˆ°çš„factor_df, å› ä¸ºfactorçš„è®¡ç®—éœ€è¦ä¹‹å‰ä¸€æ®µwindowä¸­çš„featureå€¼
    print(f'æ£€æŸ¥X_allçš„å½¢çŠ¶ {X_all.shape}')
    print(f'æ£€æŸ¥x dataset trainçš„å½¢çŠ¶ {X_dataset_train.shape}')
    print(f'æ£€æŸ¥y dataset trainçš„å½¢çŠ¶ {y_dataset_train.shape}')
    print(f'æ£€æŸ¥x allæ˜¯ä¸æ˜¯ç­‰äºtrainå’Œtestç›¸åŠ  {len(X_all)},{len(X_dataset_test)+len(X_dataset_train)}')
    print(f'æ£€æŸ¥open trainçš„å½¢çŠ¶ {open_train.shape}')
    print(f'æ£€æŸ¥close trainçš„å½¢çŠ¶ {close_train.shape}')
    print(f'æ£€æŸ¥x dataset testçš„å½¢çŠ¶ {X_dataset_test.shape}')
    print(f'æ£€æŸ¥y dataset testçš„å½¢çŠ¶ {y_dataset_test.shape}')
    print(f'æ£€æŸ¥open testçš„å½¢çŠ¶ {open_test.shape}')
    print(f'æ£€æŸ¥close testçš„å½¢çŠ¶ {close_test.shape}')

    # X_all ä¸“é—¨æ˜¯ä¸ºåšbatch predictionçš„æ—¶å€™ï¼Œè¦ç”¨X_allç”Ÿæˆtesté›†è¦ç”¨åˆ°çš„factor_df, å› ä¸ºfactorçš„è®¡ç®—éœ€è¦ä¹‹å‰ä¸€æ®µwindowä¸­çš„featureå€¼
    return X_all, X_dataset_train, y_dataset_train,ret_dataset_train, X_dataset_test, y_dataset_test,ret_dataset_test, feature_names,open_train,open_test,close_train,close_test, z.index ,ohlcva_df


def compute_transformed_series(column):
    """
    è¾“å…¥çš„æ˜¯ä¸€ä¸ªdataframeçš„ä¸€åˆ—ï¼Œseries.
    è®¡ç®—å¾—åˆ°å¦‚ä¸‹å‡ ä¸ªndarry-

    1. log_return: å–log returnã€‚
    2. log_log_returnï¼šå¯¹log_returnå†åšä¸€æ¬¡log.
    3. boxcox_transformed: ä½¿ç”¨Box-Coxå˜æ¢ã€‚
    4. yeo_johnson_transformed: ä½¿ç”¨Yeo-Johnsonå˜æ¢ã€‚
    5. winsorized_log_return: å¯¹log_returnè¿›è¡ŒWinsorizingã€‚
    6. scaled_log_return: å¯¹log_returnè¿›è¡ŒRobustScalerç¼©æ”¾ã€‚

    plotä¸€ä¸ªç›´æ–¹å›¾ï¼Œä¸Šé¢4ç§é¢œè‰²æ˜¾ç¤ºå¦‚ä¸Šå››ç±»æ•°å€¼å„è‡ªçš„ç›´æ–¹åˆ†å¸ƒå›¾.
    å¹¶ä¸”åœ¨å›¾ä¸Šç”»å‡ºå¦‚ä¸Š4ä¸ªåºåˆ—å„è‡ªçš„skewnesså’Œkurtosis
    -------

    """
    log_return = (np.log(column).diff(1).fillna(0)*1).shift(-1)
    log_return = np.where(np.isnan(log_return), 0, log_return)

    # ---------å°è¯•å¯¹log returnåšæ»šåŠ¨æ ‡å‡†åŒ–--------
    log_return = _rolling_zscore(log_return, 300)

    # è®¡ç®— log_log_return
    log_log_return = np.log(log_return + 1)
    # log_log_return_2 = np.log(np.log(column)).diff().fillna(0).shift(-1)

    # å¹³ç§»æ•°æ®ä½¿å…¶ä¸ºæ­£å€¼
    log_return_shifted = log_return - np.min(log_return) + 1
    # åº”ç”¨ Box-Cox å˜æ¢
    boxcox_transformed, _ = boxcox(log_return_shifted)

    # åº”ç”¨ Yeo-Johnson å˜æ¢
    yeo_johnson_transformed, _ = yeojohnson(log_return)

    # åº”ç”¨ Winsorizing
    winsorized_log_return = mstats.winsorize(log_return, limits=[0.05, 0.05])

    # # åº”ç”¨ RobustScaler
    # scaler = RobustScaler()
    # scaled_log_return = scaler.fit_transform(log_return).flatten()

    # ç»˜åˆ¶ç›´æ–¹å›¾
    plt.figure(figsize=(12, 6))

    # ç»˜åˆ¶ log return çš„ç›´æ–¹å›¾
    plt.hist(log_return, bins=160, alpha=0.3, color='blue', label='Log Return')

    # ç»˜åˆ¶ log_log_return çš„ç›´æ–¹å›¾
    plt.hist(log_log_return, bins=160, alpha=0.3,
             color='orange', label='Log Log Return')

    # ç»˜åˆ¶ boxcox_transformed çš„ç›´æ–¹å›¾
    plt.hist(boxcox_transformed, bins=160, alpha=0.3,
             color='green', label='Box-Cox Transformed')

    # ç»˜åˆ¶ yeo_johnson_transformed çš„ç›´æ–¹å›¾
    plt.hist(yeo_johnson_transformed, bins=160, alpha=0.3,
             color='red', label='Yeo-Johnson Transformed')

    # ç»˜åˆ¶ winsorized_log_return çš„ç›´æ–¹å›¾
    plt.hist(winsorized_log_return, bins=160, alpha=0.3,
             color='red', label='Winsorized Transformed')

    # è®¡ç®—å¹¶æ˜¾ç¤º skewness å’Œ kurtosis
    log_return_skewness = skew(log_return)
    log_return_kurtosis = kurtosis(log_return)
    log_log_return_skewness = skew(log_log_return)
    log_log_return_kurtosis = kurtosis(log_log_return)
    boxcox_skewness = skew(boxcox_transformed)
    boxcox_kurtosis = kurtosis(boxcox_transformed)
    yeo_johnson_skewness = skew(yeo_johnson_transformed)
    yeo_johnson_kurtosis = kurtosis(yeo_johnson_transformed)
    winsorized_skewness = skew(winsorized_log_return)
    winsorized_kurtosis = kurtosis(winsorized_log_return)

    # åœ¨å›¾ä¸Šæ˜¾ç¤º skewness å’Œ kurtosis
    plt.text(0.05, 0.95,
             f'Log Return Skewness: {log_return_skewness:.2f}\nLog Return Kurtosis: {log_return_kurtosis:.2f}',
             fontsize=10, transform=plt.gca().transAxes, va='top', ha='left', bbox=dict(facecolor='blue', alpha=0.5))
    plt.text(0.05, 0.85,
             f'Log Log Return Skewness: {log_log_return_skewness:.2f}\nLog Log Return Kurtosis: {log_log_return_kurtosis:.2f}',
             fontsize=10, transform=plt.gca().transAxes, va='top', ha='left', bbox=dict(facecolor='orange', alpha=0.5))
    plt.text(0.05, 0.75,
             f'Box-Cox Transformed Skewness: {boxcox_skewness:.2f}\nBox-Cox Transformed Kurtosis: {boxcox_kurtosis:.2f}',
             fontsize=10, transform=plt.gca().transAxes, va='top', ha='left', bbox=dict(facecolor='green', alpha=0.5))
    plt.text(0.05, 0.65,
             f'Yeo-Johnson Transformed Skewness: {yeo_johnson_skewness:.2f}\nYeo-Johnson Transformed Kurtosis: {yeo_johnson_kurtosis:.2f}',
             fontsize=10, transform=plt.gca().transAxes, va='top', ha='left', bbox=dict(facecolor='red', alpha=0.5))
    plt.text(0.05, 0.55,
             f'Winsorized Skewness: {winsorized_skewness:.2f}\nWinsorized Kurtosis: {winsorized_kurtosis:.2f}',
             fontsize=10, transform=plt.gca().transAxes, va='top', ha='left', bbox=dict(facecolor='purple', alpha=0.5))

    # æ·»åŠ å›¾ä¾‹å’Œæ ‡ç­¾
    plt.legend()
    plt.title('Histogram of Transformed Series')
    plt.xlabel('Value')
    plt.ylabel('Frequency')

    # æ˜¾ç¤ºå›¾å½¢
    plt.show()


def check_zscore_window_series(df, column, n_values=[50, 100, 200, 250, 300, 450, 600, 1200, 2400, 4800, 9600]):
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))

    if column == 'c':
        # è®¡ç®—åŸå§‹å¯¹æ•°æ”¶ç›Šç‡ï¼Œæ­¤æ—¶ column åº”ä¸º NumPy ndarray
        log_return = (np.log(df[column]).diff(1).fillna(0)*1).shift(-1)
        # å°† NaN å€¼æ›¿æ¢ä¸º0
        log_return = np.where(np.isnan(log_return), 0, log_return)
    else:
        log_return = df[column].values

    # ç»˜åˆ¶åŸå§‹å¯¹æ•°æ”¶ç›Šç‡çš„ç›´æ–¹å›¾
    ax1.hist(log_return, bins=100, alpha=0.5,
             color='blue', label='Original Log Return')

    # è®¡ç®—ååº¦å’Œå³°åº¦
    skewness_orig = skew(log_return)
    kurtosis_orig = kurtosis(log_return)
    ax1.text(0.01, 0.9, f'Original Skew: {skewness_orig:.2f}, Kurtosis: {kurtosis_orig:.2f}',
             transform=ax1.transAxes, fontsize=10, color='blue')

    # é¢œè‰²ç”Ÿæˆå™¨
    color_cycle = plt.cm.viridis(np.linspace(0, 1, len(n_values)))

    # è®¡ç®—å¹¶ç»˜åˆ¶æ¯ä¸ªnå€¼çš„æ»šåŠ¨æ ‡å‡†åŒ–å¯¹æ•°æ”¶ç›Šç‡
    for n, color in zip(n_values, color_cycle):
        # rolling_mean = np.convolve(log_return, np.ones(n) / n, mode='valid')
        # # å¡«å……ä½¿é•¿åº¦ä¸€è‡´
        # rolling_mean = np.concatenate(
        #     (np.full(n - 1, np.nan), rolling_mean, np.full(len(log_return) - len(rolling_mean) - (n - 1), np.nan)))
        # rolling_std = np.sqrt(np.convolve((log_return - rolling_mean) ** 2, np.ones(n) / n, mode='valid'))
        # rolling_std = np.concatenate(
        #     (np.full(n - 1, np.nan), rolling_std, np.full(len(log_return) - len(rolling_std) - (n - 1), np.nan)))
        # norm_log_return = (log_return - rolling_mean) / rolling_std

        norm_log_return = _rolling_zscore_np(log_return, n)

        # ç»˜åˆ¶ç›´æ–¹å›¾
        ax1.hist(norm_log_return, bins=100, alpha=0.5,
                 color=color, label=f'Norm Log Return n={n}')

        # è®¡ç®—ååº¦å’Œå³°åº¦
        skewness = skew(norm_log_return[~np.isnan(norm_log_return)])
        kurtosis_val = kurtosis(norm_log_return[~np.isnan(norm_log_return)])
        ax1.text(0.01, 0.8 - 0.07 * n_values.index(n),
                 f'n={n} Skew: {skewness:.2f}, Kurtosis: {kurtosis_val:.2f}',
                 transform=ax1.transAxes, fontsize=10, color=color)

    # åœ¨ç¬¬äºŒä¸ªå­å›¾ä¸Šè®¾ç½®ä¸¤ä¸ªyè½´
    ax2_2 = ax2.twinx()
    for n, color in zip(n_values, color_cycle):
        norm_log_return = _rolling_zscore_np(log_return, n)
        ax2.plot(np.arange(len(df[column])), norm_log_return.cumsum(
        ), color=color, label=f'Cumulative Norm Log Ret n={n}')

    # ç»˜åˆ¶åŸå§‹æ•°å€¼
    ax2_2.plot(np.arange(len(df[column])), df[column], color='black',
               linewidth=2, label='Original Values', alpha=0.7)
    ax2.set_ylabel('Cumulative Norm Log Ret')
    ax2_2.set_ylabel('Original Values')

    ax1.set_title('Histogram of Log Returns and Normalized Log Returns')
    ax1.set_xlabel('Log Return Value')
    ax1.set_ylabel('Frequency')
    ax1.legend()
    ax2.legend(loc='upper left')
    ax2_2.legend(loc='upper right')

    plt.tight_layout()
    plt.show()


def _rolling_zscore(x1, n=300):  # æ ‡å‡†å·®æ ‡å‡†åŒ–
    x1 = x1.flatten().astype(np.double)
    x1 = np.nan_to_num(x1)
    x1_rolling_avg = ta.SMA(x1, n)  # ä½¿ç”¨TA-Libä¸­çš„ç®€å•ç§»åŠ¨å¹³å‡å‡½æ•°SMA
    x_value = _DIVP(x1, ta.STDDEV(x1, n))
    # x_value = np.clip(x_value, -6, 6)
    return np.nan_to_num(x_value)


def _rolling_zscore_np(x1, n=300):  # æ ‡å‡†å·®æ ‡å‡†åŒ–
    x = np.asarray(x1, dtype=np.float64)
    x1 = np.nan_to_num(x1)
    x1_rolling_avg = ta.SMA(x1, n)  # ä½¿ç”¨TA-Libä¸­çš„ç®€å•ç§»åŠ¨å¹³å‡å‡½æ•°SMA
    x_value = _DIVP(x1, ta.STDDEV(x1, n))
    # x_value = np.clip(x_value, -6, 6)
    return np.nan_to_num(x_value)


def _DIVP(x1, x2):  # é›¶åˆ†æ¯ä¿æŠ¤çš„é™¤æ³•
    x1 = x1.flatten().astype(np.double)
    x2 = x2.flatten().astype(np.double)
    x = np.nan_to_num(np.where(x2 != 0, np.divide(x1, x2), 0))

    return x


def cal_ret(sym: str, freq: str, n: int) -> pd.Series:
    '''è®¡ç®—æœªæ¥nä¸ªå‘¨æœŸçš„æ”¶ç›Šç‡
    params
    sym:å“ç§
    freq:é™é¢‘å‘¨æœŸ
    n:ç¬¬å‡ ä¸ªå‘¨æœŸåçš„æ”¶ç›Šç‡'''
    z = data_load(sym)
    z = resample(z, freq)

    ret = (np.log(z.c).diff(n)*1).shift(-n)  # è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡
    ret = np.where(np.isnan(ret), 0, ret)

    # å…³é”® - å¯¹labelè¿›è¡Œäº†rolling_zscoreå¤„ç†ï¼ï¼
    ret_ = _rolling_zscore_np(ret, n)
    return ret_


def data_load_v2(sym: str = 'ETHUSDT', data_dir: str = '/Users/aming/data/ETHUSDT', start_date: str = '2025-01-01', end_date: str = '2025-12-31', 
                 timeframe: str = '1h', read_frequency: str = 'monthly',
                 file_path: Optional[str] = None) -> pd.DataFrame:
    """
    æ•°æ®è¯»å–æ¨¡å— V2 - æ”¯æŒä»å¤šç§æ—¶é—´ç²’åº¦çš„æ•°æ®æ–‡ä»¶è¯»å–
    å‚æ•°:
    sym: äº¤æ˜“å¯¹ç¬¦å·ï¼Œä¾‹å¦‚ 'BTCUSDT'
    data_dir: æ•°æ®ç›®å½•è·¯å¾„ï¼Œä¾‹å¦‚ '/Volumes/Ext-Disk/data/futures/um/monthly/klines/BTCUSDT/1m'
    start_date: èµ·å§‹æ—¥æœŸ
        - æœˆåº¦æ ¼å¼: 'YYYY-MM' (å¦‚ '2020-01')
        - æ—¥åº¦æ ¼å¼: 'YYYY-MM-DD' (å¦‚ '2020-01-01')
    end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼åŒä¸Š
    timeframe: æ—¶é—´å‘¨æœŸï¼Œé»˜è®¤ '1m'ï¼Œå¯é€‰ '5m', '1h' ç­‰
    frequency: æ•°æ®é¢‘ç‡ï¼Œ'monthly'ï¼ˆæœˆåº¦ï¼‰æˆ– 'daily'ï¼ˆæ—¥åº¦ï¼‰
    file_path: ç›´æ¥æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ .feather / .zip / .csvï¼‰ï¼ŒæŒ‡å®šåå°†å¿½ç•¥å…¶ä»–å‚æ•°
    
    è¿”å›:
    åŒ…å«æ ‡å‡†åŒ–åˆ—åçš„ DataFrame
    """

    # è§£æé¢‘ç‡å‚æ•°
    try:
        freq_enum = DataFrequency(read_frequency.lower())
    except ValueError:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é¢‘ç‡: {read_frequency}ï¼Œä»…æ”¯æŒ 'monthly' æˆ– 'daily'")
    
    # ç”Ÿæˆæ—¥æœŸèŒƒå›´
    date_list = _generate_date_range(start_date, end_date, freq_enum)
    
    # è¯»å–æ‰€æœ‰æ—¶é—´æ®µçš„æ•°æ®
    df_list = []
    success_count = 0
    failed_count = 0
    
    for date_str in date_list:
        df = _read_single_period_data(sym, date_str, data_dir, timeframe, freq_enum)
        if df is not None:
            df_list.append(df)
            success_count += 1
        else:
            failed_count += 1
    
    # æ£€æŸ¥æ˜¯å¦æˆåŠŸè¯»å–åˆ°æ•°æ®
    if not df_list:
        raise ValueError(f"æœªèƒ½æˆåŠŸè¯»å–ä»»ä½•æ•°æ®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œæ—¥æœŸèŒƒå›´\nè·¯å¾„: {data_dir}\næ—¥æœŸ: {start_date} ~ {end_date}")

    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    merged_df = pd.concat(df_list, ignore_index=True)
    
    # æ ‡å‡†åŒ–åˆ—åå’Œç´¢å¼•
    standardized_df = _standardize_dataframe_columns(merged_df)
    
    print(f"æ•°æ®æ—¶é—´èŒƒå›´: {standardized_df.index.min()} è‡³ {standardized_df.index.max()}")
    print(f"{'='*60}\n")
    
    return standardized_df

class DataFrequency(Enum):
    """æ•°æ®é¢‘ç‡æšä¸¾"""
    MONTHLY = 'monthly'  # æœˆåº¦æ•°æ®
    DAILY = 'daily'      # æ—¥åº¦æ•°æ®


def _generate_date_range(start_date: str, end_date: str, read_frequency: DataFrequency = DataFrequency.MONTHLY) -> List[str]:
    
    if read_frequency == DataFrequency.MONTHLY:
        # å…¼å®¹ 'YYYY-MM' å’Œ 'YYYY-MM-DD' ä¸¤ç§æ ¼å¼ å¦‚æœæ˜¯ 'YYYY-MM-DD' æ ¼å¼ï¼Œè‡ªåŠ¨æˆªå–ä¸º 'YYYY-MM'
        new_start_date = start_date
        new_end_date = end_date
        if len(start_date) == 10:  # 'YYYY-MM-DD' æ ¼å¼
            new_start_date = start_date[:7]
        if len(end_date) == 10:
            new_end_date = end_date[:7]
            
        start_dt = datetime.strptime(new_start_date, '%Y-%m')
        end_dt = datetime.strptime(new_end_date, '%Y-%m')
        
        date_list = []
        current_dt = start_dt
        while current_dt <= end_dt:
            date_list.append(current_dt.strftime('%Y-%m'))
            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæœˆ
            if current_dt.month == 12:
                current_dt = current_dt.replace(year=current_dt.year + 1, month=1)
            else:
                current_dt = current_dt.replace(month=current_dt.month + 1)
        
        return date_list
    
    elif read_frequency == DataFrequency.DAILY:
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        
        date_list = []
        current_dt = start_dt
        while current_dt <= end_dt:
            date_list.append(current_dt.strftime('%Y-%m-%d'))
            current_dt += timedelta(days=1)
        
        return date_list
    
def _standardize_dataframe_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ ‡å‡†åŒ– DataFrame åˆ—åå¹¶è®¾ç½®ç´¢å¼•
    
    å‚æ•°:
    df: åŸå§‹ DataFrameï¼ˆåŒ…å« Binance æ ¼å¼çš„åˆ—åï¼‰
    
    è¿”å›:
    æ ‡å‡†åŒ–åçš„ DataFrame
    """
    # å°† open_time è½¬æ¢ä¸º datetime å¹¶è®¾ç½®ä¸ºç´¢å¼•
    df = df.copy()
    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')
    df.set_index('open_time', inplace=True)

    # df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')
    # df.set_index('close_time', inplace=True)
    
    # åˆ—åæ˜ å°„ï¼šæ–°åˆ—å -> æ—§åˆ—å
    # æ–°åˆ—å: open_time,open,high,low,close,volume,close_time,quote_volume,count,taker_buy_volume,taker_buy_quote_volume,ignore
    # æ—§åˆ—å: o, h, l, c, vol, vol_ccy, trades, oi, oi_ccy, toptrader_count_lsr, toptrader_oi_lsr, count_lsr, taker_vol_lsr
    column_mapping = {
        'open': 'o',
        'high': 'h',
        'low': 'l',
        'close': 'c',
        'volume': 'vol',
        'quote_volume': 'vol_ccy',
        'count': 'trades',
        'close_time': 'close_time',
    }
    
    df = df.rename(columns=column_mapping)
    
    # é€‰æ‹©éœ€è¦çš„åˆ—ï¼Œå¯¹äºç¼ºå¤±çš„åˆ—ç”¨ 0 å¡«å……
    required_columns = [
                            'o', 'h', 'l', 'c', 
                            'vol', 
                            'vol_ccy', 
                            'trades',
                        #    'oi', 'oi_ccy', 'toptrader_count_lsr', 'toptrader_oi_lsr', 'count_lsr',
                        #    'taker_vol_lsr', 
                            'close_time', 
                            'taker_buy_volume', 
                            'taker_buy_quote_volume'
                       ]
    
    # ä¸ºç¼ºå¤±çš„åˆ—æ·»åŠ é»˜è®¤å€¼ 0
    for col in required_columns:
        if col not in df.columns:
            df[col] = 0
            print(f"âš  è­¦å‘Šï¼šåˆ— '{col}' ä¸å­˜åœ¨ï¼Œå·²å¡«å……ä¸º 0")
    
    return df[required_columns]


def _read_single_period_data(sym: str, date_str: str, data_dir: str, timeframe: str = '1m',
                             frequency: DataFrequency = DataFrequency.MONTHLY) -> Optional[pd.DataFrame]:
    
    file_base_name, feather_path, zip_path = _build_file_paths(sym, date_str, data_dir, timeframe, frequency)
    
    # # ä¼˜å…ˆè¯»å– feather
    df = _read_feather_file(feather_path)
    if df is not None:
        return df
    
    # å¦‚æœ feather ä¸å­˜åœ¨ï¼Œè¯»å– zip
    df = _read_zip_file(zip_path, file_base_name, save_feather=True)
    if df is not None:
        return df
    
    # ä¸¤ç§æ–‡ä»¶éƒ½ä¸å­˜åœ¨
    print(f"âš  è­¦å‘Šï¼šæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_base_name}")
    return None

def _build_file_paths(sym: str, date_str: str, data_dir: str, timeframe: str = '1m', 
                      frequency: DataFrequency = DataFrequency.MONTHLY) -> Tuple[str, str, str]:
    """
    æ„å»ºæ–‡ä»¶è·¯å¾„
    
    å‚æ•°:
    sym: äº¤æ˜“å¯¹ç¬¦å· ETHUSDT
    date_str: æ—¥æœŸå­—ç¬¦ä¸² 2025-01
    data_dir: /Users/aming/data/ETHUSDT/15m
    timeframe: æ—¶é—´å‘¨æœŸ (å¦‚ '1m', '5m', '1h')
    frequency: æ•°æ®é¢‘ç‡
    
    è¿”å›:
    (file_base_name, feather_path, zip_path) å…ƒç»„
    """
    if frequency == DataFrequency.MONTHLY:
        file_base_name = f"{sym}-{timeframe}-{date_str}"
    elif frequency == DataFrequency.DAILY:
        file_base_name = f"{sym}-{timeframe}-{date_str}"
    
    # /Users/aming/data/ETHUSDT/15m/ 2025/ ETHUSDT-15m-2025-01.feather
    year = date_str.split('-')[0]
    feather_path = os.path.join(f'{data_dir}/{year}', f"{file_base_name}.feather")
    zip_path = os.path.join(f'{data_dir}/{year}', f"{file_base_name}.zip")
    
    return file_base_name, feather_path, zip_path


def _read_zip_file(zip_path: str, file_base_name: str, save_feather: bool = True) -> Optional[pd.DataFrame]:
    """
    è¯»å– zip æ ¼å¼æ–‡ä»¶ï¼ˆå†…å« CSVï¼‰
    
    å‚æ•°:
    zip_path: zip æ–‡ä»¶è·¯å¾„
    file_base_name: æ–‡ä»¶åŸºç¡€åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
    save_feather: æ˜¯å¦ä¿å­˜ä¸º feather æ ¼å¼ä»¥åŠ é€Ÿåç»­è¯»å–
    
    è¿”å›:
    DataFrame æˆ– Noneï¼ˆå¦‚æœè¯»å–å¤±è´¥ï¼‰
    """
    if not os.path.exists(zip_path):
        return None
    
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            # è·å– zip ä¸­çš„ csv æ–‡ä»¶å
            csv_filename = f"{file_base_name}.csv"
            
            # if csv_filename not in zip_ref.namelist():
            #     # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ª csv æ–‡ä»¶
            #     csv_files = [f for f in zip_ref.namelist() if f.endswith('.csv')]
            #     if csv_files:
            #         csv_filename = csv_files[0]
            #     else:
            #         print(f"âœ— åœ¨ {os.path.basename(zip_path)} ä¸­æ‰¾ä¸åˆ° CSV æ–‡ä»¶")
            #         return None
            
            # è¯»å– CSV æ•°æ®
            with zip_ref.open(csv_filename) as csv_file:
                df = pd.read_csv(csv_file)
                # print(f"âœ“ æˆåŠŸè¯»å– zip: {os.path.basename(zip_path)}, è¡Œæ•°: {len(df)}")
                
                # å¯é€‰ï¼šä¿å­˜ä¸º feather æ ¼å¼ä»¥åŠ é€Ÿåç»­è¯»å–
                if save_feather:
                    feather_path = zip_path.replace('.zip', '.feather')
                    try:
                        df.to_feather(feather_path)
                        print(f"  â†’ å·²ç¼“å­˜ä¸º feather æ ¼å¼")
                    except Exception as e:
                        print(f"  â†’ ä¿å­˜ feather æ–‡ä»¶å¤±è´¥: {str(e)}")
                
                return df
    
    except Exception as e:
        print(f"âœ— è¯»å– zip æ–‡ä»¶å¤±è´¥: {os.path.basename(zip_path)}, é”™è¯¯: {str(e)}")
        return None
    
def _read_feather_file(feather_path: str) -> Optional[pd.DataFrame]:
    if not os.path.exists(feather_path):
        return None
    
    try:
        df = pd.read_feather(feather_path)
        print(f"âœ“ æˆåŠŸè¯»å– feather: {os.path.basename(feather_path)}, è¡Œæ•°: {len(df)}")
        return df
    except Exception as e:
        print(f"âœ— è¯»å– feather æ–‡ä»¶å¤±è´¥: {os.path.basename(feather_path)}, é”™è¯¯: {str(e)}")
        return None

def resample_with_offset(z: pd.DataFrame, freq: str, offset: pd.Timedelta = None, 
                        closed: str = 'left', label: str = 'left') -> pd.DataFrame:
    '''
    æ”¯æŒoffsetå‚æ•°çš„resampleå‡½æ•° - ä½¿ç”¨pandasåŸç”Ÿoffsetå‚æ•°ï¼Œé¿å…æ—¶é—´ç´¢å¼•åç§»çš„é—®é¢˜
    
    å‚æ•°:
        z: è¾“å…¥çš„DataFrameï¼Œå¿…é¡»æœ‰DatetimeIndex
        freq: é‡é‡‡æ ·é¢‘ç‡ï¼Œå¦‚ '1h', '2h', '30min'
        offset: åç§»é‡ï¼ˆpd.Timedeltaï¼‰ï¼Œç”¨äºè°ƒæ•´åˆ†æ¡¶èµ·ç‚¹
                ä¾‹å¦‚ï¼šoffset=pd.Timedelta(minutes=15) ä¼šè®©1å°æ—¶æ¡¶ä» 9:15, 10:15, 11:15... å¼€å§‹
        closed: åŒºé—´é—­åˆæ–¹å¼ï¼Œ'left' æˆ– 'right'
        label: æ ‡ç­¾ä½ç½®ï¼Œ'left' æˆ– 'right'
    
    è¿”å›:
        é‡é‡‡æ ·åçš„DataFrame
    '''
    if freq == '15m':
        return z
    
    if freq != '1min' and freq != '1m':
        z.index = pd.to_datetime(z.index)
        
        # ä½¿ç”¨pandasåŸç”Ÿçš„offsetå‚æ•°ï¼Œè€Œä¸æ˜¯åç§»ç´¢å¼•
        if offset is not None:
            z_resampled = z.resample(
                freq, 
                closed=closed, 
                label=label,
                offset=offset  # ğŸ”‘ å…³é”®ï¼šä½¿ç”¨pandasåŸç”Ÿoffsetå‚æ•°
            ).agg({
                'o': 'first',
                'h': 'max',
                'l': 'min',
                'c': 'last',
                'vol': 'sum',
                'vol_ccy': 'sum',
                'trades': 'sum',
            })
        else:
            # æ²¡æœ‰offsetæ—¶ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
            z_resampled = z.resample(freq, closed=closed, label=label).agg({
                'o': 'first',
                'h': 'max',
                'l': 'min',
                'c': 'last',
                'vol': 'sum',
                'vol_ccy': 'sum',
                'trades': 'sum',
            })
        
        # å‰å‘å¡«å……NaNå€¼
        z_resampled = z_resampled.fillna(method='ffill')
        z_resampled.columns = ['o', 'h', 'l', 'c', 'vol', 'vol_ccy', 'trades']
        
        return z_resampled
    
    return z